{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/u-adrian/PSDA/blob/main/01_Exercise/Aufgabe_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e96OGEl07oXG"
      },
      "source": [
        "# Mount Google Drive\n",
        "In this example, the data is saved in the folder of personal google drive.\n",
        "\n",
        "First you have to upload the data to your google drive, then connect the drive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skgwbdhJ2aEl"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG_U0Y9w72ze"
      },
      "source": [
        "# Install Python Packages\n",
        "Although most of the commonly used Python libraries are pre-installed, new libraries can be installed using the below packages:\n",
        "\n",
        "!pip install [package name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGHEFxAJ9oqF"
      },
      "outputs": [],
      "source": [
        "!pip install tsfresh\n",
        "# tsfresh is a python package, which automatically calculates a large number of time series characteristics (features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A67lPTNHJkGs"
      },
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boLQLTsdJjY6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "sns.set()\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['ytick.labelsize'] = \"x-large\"\n",
        "plt.rcParams['xtick.labelsize'] = \"x-large\"\n",
        "plt.rcParams['axes.labelsize'] = \"x-large\"\n",
        "plt.rcParams['figure.titlesize'] = \"x-large\"\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb00bcxu9tih"
      },
      "source": [
        "# Data Description\n",
        "C-MAPSS data set which contains turbofan engine degradation data is a widely used prognostic benchmark data for predicting the Remaining useful life (RUL). This data set is simulated by the tool Commercial Modular Aero Propulsion System Simulation (C-MAPSS) developed by NASA. Run to failure simulations were performed for engines with varying degrees of initial wear but in a healthy state. During each cycle in the simulation, one sample of all 21 sensors such as physical core speed, temperature at fan inlet and pressure at fan inlet etc will be recorded once. As the simulation progresses, the performance of the turbofan engine degrades until it loses functionality. \n",
        "\n",
        "C-MAPSS data consists of four sub-data sets with different operational conditions and fault patterns. \n",
        "\n",
        "|         Dataset        | FD001 | FD002 | FD003 | FD004 |\n",
        "|:----------------------:|:-----:|:-----:|:-----:|:-----:|\n",
        "|      Training set      |  100  |  260  |  100  |  249  |\n",
        "|        Test set        |  100  |  259  |  100  |  248  |\n",
        "| Operational conditions |   1   |   6   |   1   |   6   |\n",
        "| Fault conditions       | 1     | 1     | 2     | 2     |\n",
        "\n",
        "\n",
        "As shown Table above, each sub-data set has been split into a training set and a test set. The training sets contain sensor records for all cycles in the run to failure simulation. Unlike the training sets, the test sets only contain partial temporal sensor records which stopped at a time prior to the failure. The task is to predict the RUL of each engine in the test sets by using the training sets with the given sensor records. The corresponding RUL to test sets has been provided. With this, the performance of the model can be verified. \n",
        "\n",
        "The data provieded as text file with 26 columns of numbers, separated by spaces. Each row is a snapshot of data taken during a single operational cycle, each column is a different variable. The columns correspond to:\n",
        "\n",
        "unit number\n",
        "\n",
        "time, in cycles\n",
        "\n",
        "operational setting 1\n",
        "\n",
        "operational setting 2\n",
        "\n",
        "operational setting 3\n",
        "\n",
        "sensor measurement 1\n",
        "\n",
        "sensor measurement 2 \n",
        "\n",
        "sensor measurement 3 \n",
        "\n",
        "...\n",
        "\n",
        "sensor measurement 26"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLw-w1TDE9Zo"
      },
      "source": [
        "# Data Exploration and Preparation\n",
        "take FD001 as example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAmDXiZU2mkK"
      },
      "outputs": [],
      "source": [
        "# Load the Data\n",
        "Path_to_data = \"drive/My Drive/PSDA2020/\"\n",
        "column_name = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "               's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "               's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "\n",
        "# training data set\n",
        "train_FD001 = pd.read_table(Path_to_data+\"train_FD001.txt\", header=None, delim_whitespace=True)\n",
        "train_FD001.columns = column_name\n",
        "\n",
        "# test data set\n",
        "test_FD001 = pd.read_table(Path_to_data+\"test_FD001.txt\", header=None, delim_whitespace=True)\n",
        "test_FD001.columns = column_name\n",
        "\n",
        "# RUL for test data set\n",
        "RUL_FD001 = pd.read_table(Path_to_data+\"RUL_FD001.txt\", header=None, delim_whitespace=True)\n",
        "\n",
        "train_FD001.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2R6-hIDIJZu"
      },
      "source": [
        "In this sub dataset we have **100** engines (engine_id) which are monitored over time (cycle). Each engine had operational_settings and sensor_measurements recorded for each cycle. The RUL is the amount of cycles an engine has left before it needs maintenance. What makes this data set special is that the engines run all the way until failure, giving us precise RUL information for every engine at every point in time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7z2IzRdSIdDk"
      },
      "outputs": [],
      "source": [
        "def add_RUL(col):\n",
        "    # Reverse the cycle evolution, where remaining time of a machine is 0 at the failure.\n",
        "    # It is assumed here that the state of the machine is linearly deteriorating\n",
        "    return col[::-1]-1\n",
        "# Calculate RUL for each time point of each engine  \n",
        "train_FD001['rul'] = train_FD001[['engine_id', 'cycle']].groupby('engine_id').transform(add_RUL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQMbZSIAE1aU"
      },
      "source": [
        "**Is there any other way to define target lable (RUL) ?** \n",
        "\n",
        "**Response:**\n",
        "\n",
        "The target label RUL could also be defined through counting the rows of the engine id and subtract the cycle number of the row. This will result into a RUL column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfBy89S6P4kJ"
      },
      "outputs": [],
      "source": [
        "# Visualize the RUL curve of some engines (1,2,3,4,5,6)\n",
        "g = sns.PairGrid(data=train_FD001.reset_index().query('engine_id < 7') ,\n",
        "                 x_vars=[\"index\"],\n",
        "                 y_vars=['rul'],\n",
        "                 hue=\"engine_id\", height=3, aspect=2.5)\n",
        "\n",
        "g = g.map(plt.plot, alpha=1)\n",
        "g = g.add_legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YfEPxGjLm5K"
      },
      "outputs": [],
      "source": [
        "# Visualize some sensor curves of some engines \n",
        "g = sns.PairGrid(data=train_FD001.query('engine_id < 5') ,\n",
        "                 x_vars=[\"rul\"],\n",
        "                 y_vars=['s1','s2'],\n",
        "                 hue=\"engine_id\", height=3, aspect=2.5)\n",
        "\n",
        "g = g.map(plt.plot, alpha=1)\n",
        "g = g.add_legend()\n",
        "\n",
        "# As shown in the figure, some sensors are not related to RUL. \n",
        "# The values of some sensors change with the state of the machine. \n",
        "# Visualization can help filter features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kbeakQcTv5n"
      },
      "outputs": [],
      "source": [
        "# Distribution of maximum life cycle\n",
        "train_FD001[['engine_id', 'rul']].groupby('engine_id').apply(np.max)[\"rul\"].hist(bins=20)\n",
        "plt.xlabel(\"max life cycle\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5urHHKAeFKY7"
      },
      "source": [
        "**Can you do more visualizationï¼Ÿ Please give a simple summary or explanation for each visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzyg1Br7AYxk"
      },
      "outputs": [],
      "source": [
        "# Generate heatmap for parameter correlation\n",
        "sns.heatmap(train_FD001.corr())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwWNM2GQWE-Q"
      },
      "outputs": [],
      "source": [
        "# prepare the data and normalization\n",
        "train_y = train_FD001['rul']\n",
        "features = train_FD001.columns.drop(['engine_id', 'cycle', 'rul'])\n",
        "train_x = train_FD001[features]\n",
        "test_x = test_FD001[features]\n",
        "\n",
        "# z score normalization\n",
        "mean = train_x.mean()\n",
        "std = train_x.std()\n",
        "std.replace(0, 1, inplace=True)\n",
        "\n",
        "train_x = (train_x - mean) / std\n",
        "test_x = (test_x - mean) / std\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "x, y = shuffle(train_x, train_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNk2ycyuFrHq"
      },
      "source": [
        "**Here only the values at each time point (cycle) are used to predict the RUL. Temporal relationship is ignored. How to use Temporal relationship?**\n",
        "\n",
        "tip : Sliding Window\n",
        "\n",
        "**Response:**\n",
        "\n",
        "Temporal relationship could be used through using more than one row of an engine id at a time. This can be achieved with a sliding window approach. A fixed number of rows could be input into a model and the RUL of the last row could be used as a target for this sliding window. The sliding window could then slide over all the data of a specific engine id to calculate the RUL of the last row in the input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZPWL4IWVJFV"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alPz0At-SyVz"
      },
      "outputs": [],
      "source": [
        "# Random Forest with default Hyper parameters\n",
        "rf_model = RandomForestRegressor()\n",
        "rf_model.fit(x,y)\n",
        "rf_prediction = rf_model.predict(train_x)\n",
        "plt.plot(rf_prediction[:500])\n",
        "plt.plot(train_FD001[\"rul\"][:500])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDzgpXQcLGsC"
      },
      "outputs": [],
      "source": [
        "# Lasso model with default Hyper parameters\n",
        "ls_model = LassoCV()\n",
        "ls_model.fit(x,y)\n",
        "ls_prediction = ls_model.predict(train_x)\n",
        "plt.plot(ls_prediction[:500])\n",
        "plt.plot(train_FD001[\"rul\"][:500])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I12xg2WDK0pD"
      },
      "source": [
        "**How to tune hyperparameters and select models?** \n",
        "\n",
        "**Response:**\n",
        "\n",
        "First we define the Hyperparameters for each model. Important is, that the set of hyperparameters differs between the models.  For example for the Random Forest: depth of trees or number of estimators. For the Lasso model suitable Hyperparameters are: alphas, normalize or max_iter. Then we tune every model for their set of Hyperparameters using GridSearchCV. Afterwards we select the model that performs best.\n",
        "\n",
        "**Neural network model?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC0IxbSXDsUH"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr_model = LinearRegression()\n",
        "lr_model = lr_model.fit(x,y)\n",
        "lr_prediction = lr_model.predict(train_x)\n",
        "plt.plot(lr_prediction[:500])\n",
        "plt.plot(train_FD001[\"rul\"][:500])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hMWFKMIE_Ts"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp_model = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
        "mlp_model.fit(x, y)\n",
        "mlp_prediction = mlp_model.predict(train_x)\n",
        "plt.plot(mlp_prediction[:500])\n",
        "plt.plot(train_FD001[\"rul\"][:500])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iQ_3EzOVeAe"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie-jLvidHZ9O"
      },
      "outputs": [],
      "source": [
        "# Since only the value at one time point is used, it can be seen that a lot of data in the test set is not used\n",
        "\n",
        "test_x['engine_id'] = test_FD001['engine_id']\n",
        "test_input = []\n",
        "for id in test_x['engine_id'].unique():\n",
        "  test_input.append(test_x[test_x['engine_id']==id].iloc[-1,:-1].values)\n",
        "\n",
        "test_input = np.array(test_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ1HI43iHZ65"
      },
      "outputs": [],
      "source": [
        "# Random forest\n",
        "\n",
        "rf_test_prediction = rf_model.predict(test_input)\n",
        "\n",
        "rf_rmse = np.sqrt(mean_squared_error(rf_test_prediction, RUL_FD001.values.reshape(-1)))\n",
        "\n",
        "print(\"The RMSE of random forest on test dataset FD001 is \",rf_rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YW_6FmUHZ4T"
      },
      "outputs": [],
      "source": [
        "# Lasso model\n",
        "\n",
        "ls_test_prediction = ls_model.predict(test_input)\n",
        "\n",
        "ls_rmse = np.sqrt(mean_squared_error(ls_test_prediction, RUL_FD001.values.reshape(-1)))\n",
        "\n",
        "print(\"The RMSE of Lasso model on test dataset FD001 is \",ls_rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "to7uGGvCEZLA"
      },
      "outputs": [],
      "source": [
        "lr_test_prediction = lr_model.predict(test_input)\n",
        "\n",
        "lr_rmse = np.sqrt(mean_squared_error(lr_test_prediction, RUL_FD001.values.reshape(-1)))\n",
        "\n",
        "print(\"The RMSE of Linear Regression model on test dataset FD001 is \",lr_rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-BL7DnGFdQs"
      },
      "outputs": [],
      "source": [
        "mlp_test_prediction = mlp_model.predict(test_input)\n",
        "\n",
        "mlp_rmse = np.sqrt(mean_squared_error(mlp_test_prediction, RUL_FD001.values.reshape(-1)))\n",
        "\n",
        "print(\"The RMSE of Linear Regression model on test dataset FD001 is \",mlp_rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myaoeDxY09PE"
      },
      "source": [
        "**What is your best result? If the used model is interpretable, what other conclusions can be summarized**\n",
        "\n",
        "**Response:**\n",
        "\n",
        "The best result is provided by the MLP Regression model. The RSME is lowest with 27.27 on the test data. Unfortunately the model is hard to interpret, because of the hidden layers and many parameters. The loss is decreasing over the iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnimeImgGfth"
      },
      "outputs": [],
      "source": [
        "plt.plot(mlp_model.loss_curve_)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "C-MAPSS RUL Estimation.ipynb",
      "private_outputs": false,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "36dbedb81d7ceba0f066bf55c4ca17e4a60822311f16d36a7e88f7adfb286c67"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
